{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from numpy.random import default_rng  # a default random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def plot_hist(data, xlabel, figtitle = None, num_bins = None):\n",
    "  \"\"\" Plot the given data as a histogram.\n",
    "\n",
    "    Args:\n",
    "      data (ndarray): array with data to plot as histogram\n",
    "      xlabel (str): label of x-axis\n",
    "      figtitle (str): title of histogram plot (default is no title)\n",
    "      num_bins (int): number of bins for histogram (default is 10)\n",
    "\n",
    "    Returns:\n",
    "      count (ndarray): number of samples in each histogram bin\n",
    "      bins (ndarray): center of each histogram bin\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel('Count')\n",
    "  if num_bins is not None:\n",
    "    count, bins, _ = plt.hist(data, max(data), bins=num_bins)\n",
    "  else:\n",
    "    count, bins, _ = plt.hist(data, max(data))  # 10 bins default\n",
    "  if figtitle is not None:\n",
    "    fig.suptitle(figtitle, size=16)\n",
    "  plt.show()\n",
    "  return count, bins\n",
    "\n",
    "\n",
    "def plot_gaussian_samples_true(samples, xspace, mu, sigma, xlabel, ylabel):\n",
    "  \"\"\" Plot a histogram of the data samples on the same plot as the gaussian\n",
    "  distribution specified by the give mu and sigma values.\n",
    "\n",
    "    Args:\n",
    "      samples (ndarray): data samples for gaussian distribution\n",
    "      xspace (ndarray): x values to sample from normal distribution\n",
    "      mu (scalar): mean parameter of normal distribution\n",
    "      sigma (scalar): variance parameter of normal distribution\n",
    "      xlabel (str): the label of the x-axis of the histogram\n",
    "      ylabel (str): the label of the y-axis of the histogram\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  # num_samples = samples.shape[0]\n",
    "\n",
    "  count, bins, _ = plt.hist(samples, density=True)  # probability density function\n",
    "\n",
    "  plt.plot(xspace, norm.pdf(xspace, mu, sigma), 'r-')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_likelihoods(likelihoods, mean_vals, variance_vals):\n",
    "  \"\"\" Plot the likelihood values on a heatmap plot where the x and y axes match\n",
    "  the mean and variance parameter values the likelihoods were computed for.\n",
    "\n",
    "    Args:\n",
    "      likelihoods (ndarray): array of computed likelihood values\n",
    "      mean_vals (ndarray): array of mean parameter values for which the\n",
    "                            likelihood was computed\n",
    "      variance_vals (ndarray): array of variance parameter values for which the\n",
    "                            likelihood was computed\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(likelihoods)\n",
    "\n",
    "  cbar = ax.figure.colorbar(im, ax=ax)\n",
    "  cbar.ax.set_ylabel('log likelihood', rotation=-90, va=\"bottom\")\n",
    "\n",
    "  ax.set_xticks(np.arange(len(mean_vals)))\n",
    "  ax.set_yticks(np.arange(len(variance_vals)))\n",
    "  ax.set_xticklabels(mean_vals)\n",
    "  ax.set_yticklabels(variance_vals)\n",
    "  ax.set_xlabel('Mean')\n",
    "  ax.set_ylabel('Variance')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def posterior_plot(x, likelihood=None, prior=None,\n",
    "                   posterior_pointwise=None, ax=None):\n",
    "  \"\"\"\n",
    "  Plots normalized Gaussian distributions and posterior.\n",
    "\n",
    "    Args:\n",
    "        x (numpy array of floats):         points at which the likelihood has been evaluated\n",
    "        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`\n",
    "        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`\n",
    "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "        ax: Axis in which to plot. If None, create new axis.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "  \"\"\"\n",
    "  if likelihood is None:\n",
    "      likelihood = np.zeros_like(x)\n",
    "\n",
    "  if prior is None:\n",
    "      prior = np.zeros_like(x)\n",
    "\n",
    "  if posterior_pointwise is None:\n",
    "      posterior_pointwise = np.zeros_like(x)\n",
    "\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(x, likelihood, '-C1', linewidth=2, label='Auditory')\n",
    "  ax.plot(x, prior, '-C0', linewidth=2, label='Visual')\n",
    "  ax.plot(x, posterior_pointwise, '-C2', linewidth=2, label='Posterior')\n",
    "  ax.legend()\n",
    "  ax.set_ylabel('Probability')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  plt.show()\n",
    "\n",
    "  return ax\n",
    "\n",
    "\n",
    "def plot_classical_vs_bayesian_normal(num_points, mu_classic, var_classic,\n",
    "                                      mu_bayes, var_bayes):\n",
    "  \"\"\" Helper function to plot optimal normal distribution parameters for varying\n",
    "  observed sample sizes using both classic and Bayesian inference methods.\n",
    "\n",
    "    Args:\n",
    "      num_points (int): max observed sample size to perform inference with\n",
    "      mu_classic (ndarray): estimated mean parameter for each observed sample size\n",
    "                                using classic inference method\n",
    "      var_classic (ndarray): estimated variance parameter for each observed sample size\n",
    "                                using classic inference method\n",
    "      mu_bayes (ndarray): estimated mean parameter for each observed sample size\n",
    "                                using Bayesian inference method\n",
    "      var_bayes (ndarray): estimated variance parameter for each observed sample size\n",
    "                                using Bayesian inference method\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  xspace = np.linspace(0, num_points, num_points)\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('n data points')\n",
    "  ax.set_ylabel('mu')\n",
    "  plt.plot(xspace, mu_classic,'r-', label=\"Classical\")\n",
    "  plt.plot(xspace, mu_bayes,'b-', label=\"Bayes\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('n data points')\n",
    "  ax.set_ylabel('sigma^2')\n",
    "  plt.plot(xspace, var_classic,'r-', label=\"Classical\")\n",
    "  plt.plot(xspace, var_bayes,'b-', label=\"Bayes\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability the rat will be in area 2 after 4 transitions is: [0.     0.4311 0.    ]\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "## TODO for student\n",
    "## Fill out the following then remove\n",
    "###################################################################\n",
    "\n",
    "# Transition matrix\n",
    "transition_matrix = np.array([[ 0.2, 0.6, 0.2], [ .6, 0.3, 0.1], [0.8, 0.2, 0]])\n",
    "\n",
    "# Initial state, p0\n",
    "p0 = np.array([0, 1, 0])\n",
    "\n",
    "# Compute the probabilities 4 transitions later (use np.linalg.matrix_power to raise a matrix a power)\n",
    "p4 = p0@np.linalg.matrix_power(transition_matrix,4)\n",
    "\n",
    "# The second area is indexed as 1 (Python starts indexing at 0)\n",
    "print(f\"The probability the rat will be in area 2 after 4 transitions is: {p4[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of time spend by the rat in each of the three states is: [0.44736842 0.42105263 0.13157895]\n"
     ]
    }
   ],
   "source": [
    "# Initialize random initial distribution\n",
    "p_random = np.ones((1, 3))/3\n",
    "\n",
    "###################################################################\n",
    "## TODO for student: Fill compute the state matrix after 100 transitions\n",
    "###################################################################\n",
    "\n",
    "# Fill in the missing line to get the state matrix after 100 transitions, like above\n",
    "p_average_time_spent = p_random @ np.linalg.matrix_power(transition_matrix, 100)\n",
    "print(f\"The proportion of time spend by the rat in each of the three states is: {p_average_time_spent[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-92904.81398544145\n"
     ]
    }
   ],
   "source": [
    "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
    "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
    "  potential mean and variance values for a normal distribution\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): 1-D array with all the observed data\n",
    "      mean_val (scalar): value of mean for which to compute likelihood\n",
    "      standard_dev_val (scalar): value of variance for which to compute likelihood\n",
    "\n",
    "    Returns:\n",
    "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
    "  \"\"\"\n",
    "\n",
    "  ###################################################################\n",
    "  ## TODO for student\n",
    "  ###################################################################\n",
    "\n",
    "  # Get probability of each data point (use norm.pdf from scipy stats)\n",
    "  p_data = norm.pdf(x, mean_val, standard_dev_val)\n",
    "\n",
    "  # Compute likelihood (sum over the log of the probabilities)\n",
    "  likelihood = np.sum(np.log(p_data))\n",
    "\n",
    "  return likelihood\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Compute likelihood for a guessed mean/standard dev\n",
    "guess_mean = 4\n",
    "guess_standard_dev = .1\n",
    "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal mean estimate is: 5.2803540617183025\n",
      "The optimal standard deviation estimate is: 1.1481914209445956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\headl\\AppData\\Local\\Temp\\ipykernel_26156\\2272436908.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  return -sum(np.log(norm.pdf(x, theta[0], theta[1])))\n"
     ]
    }
   ],
   "source": [
    "# We define the function to optimise, the negative log likelihood\n",
    "def negLogLike(theta, x):\n",
    "  \"\"\" Function for computing the negative log-likelihood given the observed data\n",
    "      and given parameter values stored in theta.\n",
    "\n",
    "      Args:\n",
    "        theta (ndarray): normal distribution parameters\n",
    "                        (mean is theta[0], standard deviation is theta[1])\n",
    "        x (ndarray): array with observed data points\n",
    "\n",
    "      Returns:\n",
    "        Calculated negative Log Likelihood value!\n",
    "  \"\"\"\n",
    "  ###################################################################\n",
    "  ## TODO for students: Compute the negative log-likelihood value for the\n",
    "  ## given observed data values and parameters (theta)\n",
    "  # Fill out the following then remove\n",
    "  \n",
    "  ###################################################################\n",
    "  return -sum(np.log(norm.pdf(x, theta[0], theta[1])))\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size=(n_samples, ))\n",
    "\n",
    "# Define bounds, var has to be positive\n",
    "bnds = ((None, None), (0, None))\n",
    "\n",
    "# Optimize with scipy!\n",
    "optimal_parameters = sp.optimize.minimize(negLogLike, (2, 2), args=x, bounds=bnds)\n",
    "print(f\"The optimal mean estimate is: {optimal_parameters.x[0]}\")\n",
    "print(f\"The optimal standard deviation estimate is: {optimal_parameters.x[1]}\")\n",
    "\n",
    "# optimal_parameters contains a lot of information about the optimization,\n",
    "# but we mostly want the mean and standard deviation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
